{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zYPnKxIhFGVJ",
   "metadata": {
    "id": "zYPnKxIhFGVJ"
   },
   "source": [
    "# Build homogeneous dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x7McZwR_FRta",
   "metadata": {
    "id": "x7McZwR_FRta"
   },
   "source": [
    "## Setting up environment\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jX5i4Qjs7MIl",
   "metadata": {
    "id": "jX5i4Qjs7MIl"
   },
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5okfWmvKEI3Z",
   "metadata": {
    "id": "5okfWmvKEI3Z"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import pickle\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from box import Box\n",
    "\n",
    "from util.postgres import create_sqlalchemy_engine\n",
    "from util.homogeneous.dataset import DatasetEuCoHM, assert_bidirectional_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQ_xqBiFn97Q",
   "metadata": {
    "id": "bQ_xqBiFn97Q"
   },
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9r1K6ezsn97Q",
   "metadata": {
    "id": "9r1K6ezsn97Q"
   },
   "outputs": [],
   "source": [
    "# -------------------- GLOBAL VARIABLES --------------------\n",
    "PATH_TO_CONFIG_FILE = '../config.yaml'\n",
    "\n",
    "# -------------------- LOAD CONFIGURATION --------------------\n",
    "# Load the configuration file\n",
    "config = Box.from_yaml(filename=PATH_TO_CONFIG_FILE)\n",
    "\n",
    "num_train = 0.7             # Percentage of data used for training\n",
    "num_bootstraping = 10\n",
    "pg_engine = create_sqlalchemy_engine(\n",
    "    username=config.POSTGRES.USERNAME,\n",
    "    password=config.POSTGRES.PASSWORD,\n",
    "    host=config.POSTGRES.HOST,\n",
    "    port=config.POSTGRES.PORT,\n",
    "    database=config.POSTGRES.DATABASE,\n",
    "    schema=config.POSTGRES.SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ISUVX4hvFMKM",
   "metadata": {
    "id": "ISUVX4hvFMKM"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6506c6d2-2348-413b-b026-85442cf1398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_testing(data):\n",
    "    # Test: check that the number of elements in the positive edge index equals to the number of elements in the negative edge index\n",
    "    assert data.test_pos_edge_index.numel() == data.test_neg_edge_index.numel()\n",
    "    \n",
    "    # Test: check that all positive edges are bidirectional\n",
    "    assert_bidirectional_edges(edges=data.train_pos_edge_index)\n",
    "    assert_bidirectional_edges(edges=data.test_pos_edge_index)\n",
    "    print('All tests passed')\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    dataset_save_filepath = f'../data/{dataset.get_dataset_name()}.pkl'\n",
    "    # Before saving the dataset, we need to close the engine to connect to Postgres DB.\n",
    "    dataset.close_engine()\n",
    "    # Save the dataset\n",
    "    with open(dataset_save_filepath, 'wb') as output:\n",
    "        pickle.dump(dataset, output, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'Dataset saved to {dataset_save_filepath}')\n",
    "\n",
    "def build_dataset(use_periodical_embedding_decay: bool,\n",
    "                  use_top_keywords: bool,\n",
    "                  num_train: float, \n",
    "                  bootstrap_id: int):\n",
    "    # Build the homogeneous graph\n",
    "    data: Data\n",
    "    author_node_id_map: dict\n",
    "    author_id_map: dict\n",
    "    dataset: DatasetEuCoHM = DatasetEuCoHM(\n",
    "        pg_engine=pg_engine,\n",
    "        num_train=num_train,\n",
    "        use_periodical_embedding_decay=use_periodical_embedding_decay,\n",
    "        use_top_keywords=use_top_keywords,\n",
    "        bootstrap_id=bootstrap_id\n",
    "    )\n",
    "    data, author_node_id_map, author_id_map = dataset.build_homogeneous_graph()\n",
    "    return dataset, data, author_node_id_map, author_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5880965f-4ee5-4ac6-8ab2-ed62c6370047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset configuration {'use_periodical_embedding_decay': False, 'use_top_keywords': False, 'num_train': 0.7}...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "build_dataset() missing 1 required positional argument: 'bootstrap_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing dataset configuration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Building dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dataset, data, author_node_id_map, author_id_map \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_periodical_embedding_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_periodical_embedding_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_top_keywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_top_keywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Unit testing\u001b[39;00m\n\u001b[1;32m     15\u001b[0m unit_testing(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: build_dataset() missing 1 required positional argument: 'bootstrap_id'"
     ]
    }
   ],
   "source": [
    "# Uncomment the ones you need to rebuild\n",
    "dataset_configurations = [\n",
    "    {'use_periodical_embedding_decay': False, 'use_top_keywords': False, 'num_train': num_train}\n",
    "]\n",
    "\n",
    "for conf in dataset_configurations:\n",
    "    print(f'Processing dataset configuration {conf}...')\n",
    "    # Building dataset\n",
    "    dataset, data, author_node_id_map, author_id_map = build_dataset(\n",
    "        use_periodical_embedding_decay=conf['use_periodical_embedding_decay'],\n",
    "        use_top_keywords=conf['use_top_keywords'],\n",
    "        num_train=conf['num_train']\n",
    "    )\n",
    "    # Unit testing\n",
    "    unit_testing(data)\n",
    "\n",
    "    # Save dataset\n",
    "    save_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81871ab4-e87f-42c5-b993-b0f3181d2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the ones you need to rebuild\n",
    "dataset_configurations = [\n",
    "    {'use_periodical_embedding_decay': False, 'use_top_keywords': False, 'num_train': num_train},\n",
    "    # {'use_periodical_embedding_decay': True, 'use_top_keywords': False, 'num_train': num_train},\n",
    "    # {'use_periodical_embedding_decay': False, 'use_top_keywords': True, 'num_train': num_train},\n",
    "    # {'use_periodical_embedding_decay': True, 'use_top_keywords': True, 'num_train': num_train},\n",
    "    # {'use_periodical_embedding_decay': True, 'use_top_keywords': False, 'num_train': 1.0},\n",
    "]\n",
    "\n",
    "for conf in dataset_configurations:\n",
    "    print(f'Processing dataset configuration {conf}...')\n",
    "    # Building dataset\n",
    "    for bootstrap_id in range(num_bootstraping):\n",
    "        dataset, data, author_node_id_map, author_id_map = build_dataset(\n",
    "            use_periodical_embedding_decay=conf['use_periodical_embedding_decay'],\n",
    "            use_top_keywords=conf['use_top_keywords'],\n",
    "            num_train=conf['num_train'],\n",
    "            bootstrap_id=bootstrap_id\n",
    "        )\n",
    "        # Unit testing\n",
    "        unit_testing(data)\n",
    "    \n",
    "        # Save dataset\n",
    "        save_dataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Collaboration Recommender 2.0 (Heterogeneous)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
